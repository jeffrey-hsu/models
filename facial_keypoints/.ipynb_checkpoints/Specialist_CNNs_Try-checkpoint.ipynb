{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialist Convolutional Neural Network Models\n",
    "\n",
    "In this notebook we build several CNN models each trained to predict different facial keypoint sets. The rationale for such approach is that each keypoint contains different number of label observations. Some images show face on the side, thus not all keypoints are revealed. In order to fully utilize labels, other than just examples where full set of keypoints are marked, keypoints close to each other and which number of observations are relatively close are trained with a CNN. This gives us several CNN models. The final prediction is done via all CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:556: UserWarning: Theano flag device=gpu* (old gpu back-end) only support floatX=float32. You have floatX=float64. Use the new gpu back-end with device=cuda* for that value of floatX.\n",
      "  warnings.warn(msg)\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "float64\n",
      "3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "from six.moves import cPickle\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.pool import pool_2d\n",
    "\n",
    "import conv_net_helper as helper\n",
    "\n",
    "print(theano.config.device)\n",
    "print(theano.config.floatX)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data and Create Training and Dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6042\n",
      "Number of dev examples: 1007\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \"../../../data/facial_keypoints/training.csv\"\n",
    "TEST = \"../../../data/facial_keypoints/test.csv\"\n",
    "\n",
    "rawdata = helper.load_data(TRAIN)\n",
    "X, Y = helper.loadXY(rawdata)\n",
    "num_dev = int(len(X)/7)\n",
    "train_data, train_labels = X[num_dev:], Y[num_dev:]\n",
    "dev_data, dev_labels = X[:num_dev], Y[:num_dev]\n",
    "\n",
    "print(\"Number of training examples:\", len(train_data))\n",
    "print(\"Number of dev examples:\", len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column names of keypoint sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eye_ct', 'eye_cr', 'eyebrow', 'nose', 'mouth_cr', 'mouth_ct_top', 'mouth_ct_bottom'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_keypoints = (rawdata.columns.values).tolist()[:-1]\n",
    "cols_dict = {\n",
    "    \"eye_ct\" : [\"left_eye_center_x\", \"left_eye_center_y\",\n",
    "                \"right_eye_center_x\", \"right_eye_center_y\"],\n",
    "    \"eye_cr\" : [\"left_eye_inner_corner_x\", \"left_eye_inner_corner_y\",\n",
    "                \"left_eye_outer_corner_x\", \"left_eye_outer_corner_y\",\n",
    "                \"right_eye_inner_corner_x\", \"right_eye_inner_corner_y\",\n",
    "                \"right_eye_outer_corner_x\", \"right_eye_outer_corner_y\"],\n",
    "    \"eyebrow\" : [\"left_eyebrow_inner_end_x\", \"left_eyebrow_inner_end_y\",\n",
    "                 \"left_eyebrow_outer_end_x\", \"left_eyebrow_outer_end_y\",\n",
    "                 \"right_eyebrow_inner_end_x\", \"right_eyebrow_inner_end_y\",\n",
    "                 \"right_eyebrow_outer_end_x\", \"right_eyebrow_outer_end_y\"],\n",
    "    \"nose\" : [\"nose_tip_x\", \"nose_tip_y\"],\n",
    "    \"mouth_cr\" : [\"mouth_left_corner_x\", \"mouth_left_corner_y\",\n",
    "                           \"mouth_right_corner_x\", \"mouth_right_corner_y\"],\n",
    "    \"mouth_ct_top\" : [\"mouth_center_top_lip_x\", \"mouth_center_top_lip_y\"],\n",
    "    \"mouth_ct_bottom\" : [\"mouth_center_bottom_lip_x\", \"mouth_center_bottom_lip_y\"]\n",
    "}\n",
    "cols_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Traing Data - for keypoint sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_sep = [0 for i in cols_dict.keys()]\n",
    "train_labels_sep = [0 for i in cols_dict.keys()]\n",
    "for i, key in enumerate(cols_dict.keys()):\n",
    "    train_data_sep[i], train_labels_sep[i] = helper.subset_data(train_data,\n",
    "                                                                train_labels,\n",
    "                                                                full_keypoints,\n",
    "                                                                cols_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Dev Data - for keypoint sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_data_sep = [0 for i in cols_dict.keys()]\n",
    "dev_labels_sep = [0 for i in cols_dict.keys()]\n",
    "for i, key in enumerate(cols_dict.keys()):\n",
    "    dev_data_sep[i], dev_labels_sep[i] = helper.subset_data(dev_data,\n",
    "                                                                dev_labels,\n",
    "                                                                full_keypoints,\n",
    "                                                                cols_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye  Center Prediction\n",
    "\n",
    "- eye_ct_data\n",
    "- eye_ct_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[0][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "eye_ct_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                   featureMapLayers = [32,64,128,256,512],\n",
    "                                   numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                   imageWidth = 96, poolingSize = 2,\n",
    "                                   train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[0] = helper.load_2d_images(train_data_sep[0], 96)\n",
    "dev_data_sep[0] = helper.load_2d_images(dev_data_sep[0], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6028, 1, 96, 96)\n",
      "(1005, 1, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "## Check if the reshape works\n",
    "print(train_data_sep[0].shape)\n",
    "print(dev_data_sep[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 / 3\n",
      "training time: 511.979686499 s, ----- loss: 0.187469573096 , RMSE: 20.7829231922\n",
      "validation loss: 0.194138256098 , val RMSE: 21.1493390452\n",
      "\n",
      "Epoch: 2 / 3\n",
      "training time: 514.298957825 s, ----- loss: 0.187469525536 , RMSE: 20.7829205559\n",
      "validation loss: 0.194138255486 , val RMSE: 21.1493390119\n",
      "\n",
      "Epoch: 3 / 3\n",
      "training time: 513.109895229 s, ----- loss: 0.187469515107 , RMSE: 20.7829199779\n",
      "validation loss: 0.194138254874 , val RMSE: 21.1493389786\n"
     ]
    }
   ],
   "source": [
    "eye_ct_train_result, eye_ct_val_result = eye_ct_cnn.SGD(train_data_sep[0],\n",
    "                                                        train_labels_sep[0],\n",
    "                                                        update_rule=\"backprop\",\n",
    "                                                        epochs = 3,\n",
    "                                                        miniBatchSize = 100,\n",
    "                                                        learning_rate = 0.01,\n",
    "                                                        validation = [dev_data_sep[0],\n",
    "                                                                     dev_labels_sep[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in range(len(eye_ct_cnn.params)):\n",
    "    filename = \"eye_ct_layer\" + str(layer+1)\n",
    "    helper.save_layer_params(eye_ct_cnn.params[layer], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0039484 , -0.01108056,  0.00097547, ...,  0.00673776,\n",
       "        -0.00781311, -0.00604274],\n",
       "       [-0.01168752,  0.00146486,  0.00517273, ...,  0.00331349,\n",
       "        -0.01609198,  0.00706688],\n",
       "       [-0.00588158,  0.00856659, -0.00125814, ..., -0.00328748,\n",
       "         0.0118611 ,  0.00234903],\n",
       "       ..., \n",
       "       [-0.01497844, -0.00012292, -0.01610636, ...,  0.00627014,\n",
       "         0.00585229, -0.00109457],\n",
       "       [-0.01884747, -0.0079815 , -0.01920352, ..., -0.00610651,\n",
       "        -0.00692251,  0.00126688],\n",
       "       [-0.00646328, -0.00138179, -0.00709273, ...,  0.02022066,\n",
       "         0.00733202, -0.00276212]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.load_saved_params(\"eye_ct_layer9_weights\").get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkyb4gGM3+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3a7723a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_performance(train_result, validation_result):\n",
    "    plt.plot(train_result[:,1], linewidth=3, label=\"train\")\n",
    "    plt.plot(validation_result[:,0], linewidth=3, label=\"validation\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.ylim(1e-4, 1e-2)\n",
    "    #plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(eye_ct_train_result, eye_ct_val_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eye center -- Try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 / 50\n",
      "training time: 528.601392031 s, ----- loss: 0.193162040013 , RMSE: 21.0960977479\n",
      "validation loss: 0.195425575984 , val RMSE: 21.2193432289\n",
      "\n",
      "Epoch: 2 / 50\n",
      "training time: 532.699882269 s, ----- loss: 0.193162165994 , RMSE: 21.0961046274\n",
      "validation loss: 0.195425572416 , val RMSE: 21.2193430352\n",
      "\n",
      "Epoch: 3 / 50\n",
      "training time: 523.758442402 s, ----- loss: 0.193162270719 , RMSE: 21.0961103462\n",
      "validation loss: 0.195425568824 , val RMSE: 21.2193428402\n",
      "\n",
      "Epoch: 4 / 50\n",
      "training time: 526.534879684 s, ----- loss: 0.193162053155 , RMSE: 21.0960984656\n",
      "validation loss: 0.19542556521 , val RMSE: 21.219342644\n",
      "\n",
      "Epoch: 5 / 50\n",
      "training time: 521.698043585 s, ----- loss: 0.193162108023 , RMSE: 21.0961014618\n",
      "validation loss: 0.195425561534 , val RMSE: 21.2193424444\n",
      "\n",
      "Epoch: 6 / 50\n",
      "training time: 522.065946817 s, ----- loss: 0.193162170144 , RMSE: 21.096104854\n",
      "validation loss: 0.195425557818 , val RMSE: 21.2193422427\n",
      "\n",
      "Epoch: 7 / 50\n",
      "training time: 524.203859806 s, ----- loss: 0.19316219708 , RMSE: 21.0961063249\n",
      "validation loss: 0.195425554075 , val RMSE: 21.2193420395\n",
      "\n",
      "Epoch: 8 / 50\n",
      "training time: 531.202541351 s, ----- loss: 0.193161862414 , RMSE: 21.0960880497\n",
      "validation loss: 0.195425550313 , val RMSE: 21.2193418352\n",
      "\n",
      "Epoch: 9 / 50\n",
      "training time: 522.63704133 s, ----- loss: 0.193161609217 , RMSE: 21.0960742233\n",
      "validation loss: 0.195425546501 , val RMSE: 21.2193416283\n",
      "\n",
      "Epoch: 10 / 50\n",
      "training time: 522.356220007 s, ----- loss: 0.193162084696 , RMSE: 21.0961001879\n",
      "validation loss: 0.19542554262 , val RMSE: 21.2193414176\n",
      "\n",
      "Epoch: 11 / 50\n",
      "training time: 521.46859622 s, ----- loss: 0.193161945568 , RMSE: 21.0960925905\n",
      "validation loss: 0.195425538716 , val RMSE: 21.2193412056\n",
      "\n",
      "Epoch: 12 / 50\n",
      "training time: 521.382940531 s, ----- loss: 0.193162148922 , RMSE: 21.0961036951\n",
      "validation loss: 0.195425534786 , val RMSE: 21.2193409923\n",
      "\n",
      "Epoch: 13 / 50\n",
      "training time: 523.889650583 s, ----- loss: 0.193161884188 , RMSE: 21.0960892388\n",
      "validation loss: 0.195425530766 , val RMSE: 21.2193407741\n",
      "\n",
      "Epoch: 14 / 50\n",
      "training time: 524.601248503 s, ----- loss: 0.193161750261 , RMSE: 21.0960819254\n",
      "validation loss: 0.195425526765 , val RMSE: 21.2193405568\n",
      "\n",
      "Epoch: 15 / 50\n",
      "training time: 521.902107 s, ----- loss: 0.193161999607 , RMSE: 21.0960955415\n",
      "validation loss: 0.195425522716 , val RMSE: 21.219340337\n",
      "\n",
      "Epoch: 16 / 50\n",
      "training time: 524.287236691 s, ----- loss: 0.193161803698 , RMSE: 21.0960848434\n",
      "validation loss: 0.195425518575 , val RMSE: 21.2193401122\n",
      "\n",
      "Epoch: 17 / 50\n",
      "training time: 524.552388906 s, ----- loss: 0.193161894234 , RMSE: 21.0960897873\n",
      "validation loss: 0.195425514444 , val RMSE: 21.2193398879\n",
      "\n",
      "Epoch: 18 / 50\n",
      "training time: 521.714756727 s, ----- loss: 0.193161705616 , RMSE: 21.0960794874\n",
      "validation loss: 0.195425510258 , val RMSE: 21.2193396607\n",
      "\n",
      "Epoch: 19 / 50\n",
      "training time: 524.31024909 s, ----- loss: 0.19316193756 , RMSE: 21.0960921533\n",
      "validation loss: 0.195425506029 , val RMSE: 21.2193394311\n",
      "\n",
      "Epoch: 20 / 50\n",
      "training time: 523.957644939 s, ----- loss: 0.193161623207 , RMSE: 21.0960749873\n",
      "validation loss: 0.195425501744 , val RMSE: 21.2193391984\n",
      "\n",
      "Epoch: 21 / 50\n",
      "training time: 522.716551542 s, ----- loss: 0.193161859072 , RMSE: 21.0960878672\n",
      "validation loss: 0.1954254974 , val RMSE: 21.2193389626\n",
      "\n",
      "Epoch: 22 / 50\n",
      "training time: 525.270279646 s, ----- loss: 0.193161782075 , RMSE: 21.0960836626\n",
      "validation loss: 0.19542549302 , val RMSE: 21.2193387248\n",
      "\n",
      "Epoch: 23 / 50\n",
      "training time: 523.081432819 s, ----- loss: 0.193161956694 , RMSE: 21.0960931981\n",
      "validation loss: 0.195425488599 , val RMSE: 21.2193384848\n",
      "\n",
      "Epoch: 24 / 50\n",
      "training time: 527.434898853 s, ----- loss: 0.193161468114 , RMSE: 21.096066518\n",
      "validation loss: 0.195425484134 , val RMSE: 21.2193382424\n",
      "\n",
      "Epoch: 25 / 50\n",
      "training time: 526.213949203 s, ----- loss: 0.193161513654 , RMSE: 21.0960690049\n",
      "validation loss: 0.195425479595 , val RMSE: 21.2193379959\n",
      "\n",
      "Epoch: 26 / 50\n",
      "training time: 538.68194437 s, ----- loss: 0.193161579 , RMSE: 21.0960725732\n",
      "validation loss: 0.195425474984 , val RMSE: 21.2193377456\n",
      "\n",
      "Epoch: 27 / 50\n",
      "training time: 534.419144154 s, ----- loss: 0.193161469852 , RMSE: 21.096066613\n",
      "validation loss: 0.195425470362 , val RMSE: 21.2193374947\n",
      "\n",
      "Epoch: 28 / 50\n",
      "training time: 517.350951433 s, ----- loss: 0.193161736414 , RMSE: 21.0960811692\n",
      "validation loss: 0.195425465615 , val RMSE: 21.219337237\n",
      "\n",
      "Epoch: 29 / 50\n",
      "training time: 503.232494354 s, ----- loss: 0.19316167616 , RMSE: 21.0960778789\n",
      "validation loss: 0.195425460827 , val RMSE: 21.219336977\n",
      "\n",
      "Epoch: 30 / 50\n",
      "training time: 502.154889107 s, ----- loss: 0.193161534169 , RMSE: 21.0960701251\n",
      "validation loss: 0.19542545602 , val RMSE: 21.2193367161\n",
      "\n",
      "Epoch: 31 / 50\n",
      "training time: 501.759007692 s, ----- loss: 0.193161152024 , RMSE: 21.0960492572\n",
      "validation loss: 0.195425451103 , val RMSE: 21.2193364491\n",
      "\n",
      "Epoch: 32 / 50\n",
      "training time: 502.216660738 s, ----- loss: 0.193161665235 , RMSE: 21.0960772823\n",
      "validation loss: 0.195425446164 , val RMSE: 21.219336181\n",
      "\n",
      "Epoch: 33 / 50\n",
      "training time: 498.792489052 s, ----- loss: 0.193161123645 , RMSE: 21.0960477075\n",
      "validation loss: 0.195425441145 , val RMSE: 21.2193359085\n",
      "\n",
      "Epoch: 34 / 50\n",
      "training time: 497.933381796 s, ----- loss: 0.19316109944 , RMSE: 21.0960463858\n",
      "validation loss: 0.195425436081 , val RMSE: 21.2193356336\n",
      "\n",
      "Epoch: 35 / 50\n",
      "training time: 496.984132767 s, ----- loss: 0.19316152316 , RMSE: 21.096069524\n",
      "validation loss: 0.19542543099 , val RMSE: 21.2193353572\n",
      "\n",
      "Epoch: 36 / 50\n",
      "training time: 497.923448324 s, ----- loss: 0.193161908329 , RMSE: 21.096090557\n",
      "validation loss: 0.195425425806 , val RMSE: 21.2193350758\n",
      "\n",
      "Epoch: 37 / 50\n",
      "training time: 498.445738554 s, ----- loss: 0.193161441913 , RMSE: 21.0960650873\n",
      "validation loss: 0.195425420546 , val RMSE: 21.2193347902\n",
      "\n",
      "Epoch: 38 / 50\n",
      "training time: 499.629440308 s, ----- loss: 0.193161565587 , RMSE: 21.0960718408\n",
      "validation loss: 0.19542541523 , val RMSE: 21.2193345016\n",
      "\n",
      "Epoch: 39 / 50\n",
      "training time: 499.429672956 s, ----- loss: 0.193161245807 , RMSE: 21.0960543785\n",
      "validation loss: 0.195425409826 , val RMSE: 21.2193342082\n",
      "\n",
      "Epoch: 40 / 50\n",
      "training time: 499.070521832 s, ----- loss: 0.193161361519 , RMSE: 21.0960606972\n",
      "validation loss: 0.195425404345 , val RMSE: 21.2193339107\n",
      "\n",
      "Epoch: 41 / 50\n",
      "training time: 501.549981594 s, ----- loss: 0.19316124675 , RMSE: 21.09605443\n",
      "validation loss: 0.195425398814 , val RMSE: 21.2193336104\n",
      "\n",
      "Epoch: 42 / 50\n",
      "training time: 514.884777546 s, ----- loss: 0.193160910221 , RMSE: 21.096036053\n",
      "validation loss: 0.195425393209 , val RMSE: 21.2193333061\n",
      "\n",
      "Epoch: 43 / 50\n",
      "training time: 504.27523756 s, ----- loss: 0.193161033689 , RMSE: 21.0960427952\n",
      "validation loss: 0.195425387492 , val RMSE: 21.2193329957\n",
      "\n",
      "Epoch: 44 / 50\n",
      "training time: 504.38713026 s, ----- loss: 0.193160954925 , RMSE: 21.0960384941\n",
      "validation loss: 0.195425381663 , val RMSE: 21.2193326792\n",
      "\n",
      "Epoch: 45 / 50\n",
      "training time: 504.346273422 s, ----- loss: 0.193160810845 , RMSE: 21.0960306263\n",
      "validation loss: 0.195425375809 , val RMSE: 21.2193323614\n",
      "\n",
      "Epoch: 46 / 50\n",
      "training time: 504.469125748 s, ----- loss: 0.193161025178 , RMSE: 21.0960423305\n",
      "validation loss: 0.195425369874 , val RMSE: 21.2193320392\n",
      "\n",
      "Epoch: 47 / 50\n",
      "training time: 503.506498337 s, ----- loss: 0.193161139532 , RMSE: 21.0960485751\n",
      "validation loss: 0.195425363908 , val RMSE: 21.2193317153\n",
      "\n",
      "Epoch: 48 / 50\n",
      "training time: 504.40993309 s, ----- loss: 0.19316085844 , RMSE: 21.0960332254\n",
      "validation loss: 0.195425357781 , val RMSE: 21.2193313826\n",
      "\n",
      "Epoch: 49 / 50\n",
      "training time: 505.208776474 s, ----- loss: 0.193160778158 , RMSE: 21.0960288414\n",
      "validation loss: 0.195425351583 , val RMSE: 21.2193310462\n",
      "\n",
      "Epoch: 50 / 50\n",
      "training time: 509.81259656 s, ----- loss: 0.193160927444 , RMSE: 21.0960369935\n",
      "validation loss: 0.195425345304 , val RMSE: 21.2193307053\n"
     ]
    }
   ],
   "source": [
    "eye_ct_train_result, eye_ct_val_result = eye_ct_cnn.SGD(train_data_sep[0],\n",
    "                                                        train_labels_sep[0],\n",
    "                                                        update_rule=\"backprop\",\n",
    "                                                        epochs = 50,\n",
    "                                                        miniBatchSize = 100,\n",
    "                                                        learning_rate = 0.03,\n",
    "                                                        validation = [dev_data_sep[0],\n",
    "                                                                      dev_labels_sep[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye  Corner Prediction\n",
    "\n",
    "- eye_cr_data\n",
    "- eye_cr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[1][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "eye_cr_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                   featureMapLayers = [32,64,128,256,512],\n",
    "                                   numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                   imageWidth = 96, poolingSize = 2,\n",
    "                                   train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[1] = helper.load_2d_images(train_data_sep[1], 96)\n",
    "dev_data_sep[1] = helper.load_2d_images(dev_data_sep[1], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eye_cr_train_result, eye_cr_val_result = eye_cr_cnn.SGD(train_data_sep[1],\n",
    "                                                        train_labels_sep[1],\n",
    "                                                        update_rule=\"rmsprop\",\n",
    "                                                        epochs = 30,\n",
    "                                                        miniBatchSize = 100,\n",
    "                                                        learning_rate = 0.03,\n",
    "                                                        validation = [dev_data_sep[1],\n",
    "                                                                     dev_labels_sep[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eyebrow Prediction\n",
    "\n",
    "- eyebrow_data\n",
    "- eyebrow_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[2][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "eyebrow_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                   featureMapLayers = [32,64,128,256,512],\n",
    "                                   numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                   imageWidth = 96, poolingSize = 2,\n",
    "                                   train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[2] = helper.load_2d_images(train_data_sep[2], 96)\n",
    "dev_data_sep[2] = helper.load_2d_images(dev_data_sep[2], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eyebrow_train_result, eyebrow_val_result = eyebrow_cnn.SGD(train_data_sep[2],\n",
    "                                                        train_labels_sep[2],\n",
    "                                                        update_rule=\"rmsprop\",\n",
    "                                                        epochs = 30,\n",
    "                                                        miniBatchSize = 100,\n",
    "                                                        learning_rate = 0.03,\n",
    "                                                        validation = [dev_data_sep[2],\n",
    "                                                                     dev_labels_sep[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nose Prediction\n",
    "\n",
    "- nose_data\n",
    "- nose_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[3][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "nose_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                   featureMapLayers = [32,64,128,256,512],\n",
    "                                   numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                   imageWidth = 96, poolingSize = 2,\n",
    "                                   train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[3] = helper.load_2d_images(train_data_sep[3], 96)\n",
    "dev_data_sep[3] = helper.load_2d_images(dev_data_sep[3], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nose_train_result, nose_val_result = nose_cnn.SGD(train_data_sep[3],\n",
    "                                                  train_labels_sep[3],\n",
    "                                                  update_rule=\"rmsprop\",\n",
    "                                                  epochs = 30,\n",
    "                                                  miniBatchSize = 100,\n",
    "                                                  learning_rate = 0.03,\n",
    "                                                  validation = [dev_data_sep[3],\n",
    "                                                                dev_labels_sep[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouth Corner Prediction\n",
    "\n",
    "- mouth_cr_data\n",
    "- mouth_cr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[4][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "mouse_cr_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                     featureMapLayers = [32,64,128,256,512],\n",
    "                                     numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                     imageWidth = 96, poolingSize = 2,\n",
    "                                     train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[4] = helper.load_2d_images(train_data_sep[4], 96)\n",
    "dev_data_sep[4] = helper.load_2d_images(dev_data_sep[4], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mouse_cr_train_result, mouse_cr_val_result = mouse_cr_cnn.SGD(train_data_sep[4],\n",
    "                                                          train_labels_sep[4],\n",
    "                                                          update_rule=\"rmsprop\",\n",
    "                                                          epochs = 30,\n",
    "                                                          miniBatchSize = 100,\n",
    "                                                          learning_rate = 0.03,\n",
    "                                                          validation = [dev_data_sep[4],\n",
    "                                                                        dev_labels_sep[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouth Center Top Prediction\n",
    "\n",
    "- mouth_ct_top_data\n",
    "- mouth_ct_top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[5][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "mouse_ct_top_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                     featureMapLayers = [32,64,128,256,512],\n",
    "                                     numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                     imageWidth = 96, poolingSize = 2,\n",
    "                                     train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[5] = helper.load_2d_images(train_data_sep[5], 96)\n",
    "dev_data_sep[5] = helper.load_2d_images(dev_data_sep[5], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mouse_ct_top_train_result, mouse_ct_top_val_result = mouse_ct_top_cnn.SGD(train_data_sep[5],\n",
    "                                                          train_labels_sep[5],\n",
    "                                                          update_rule=\"rmsprop\",\n",
    "                                                          epochs = 30,\n",
    "                                                          miniBatchSize = 100,\n",
    "                                                          learning_rate = 0.03,\n",
    "                                                          validation = [dev_data_sep[5],\n",
    "                                                                        dev_labels_sep[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mouth Center Bottom Prediction\n",
    "\n",
    "- mouth_ct_bottom_data\n",
    "- mouth_ct_bottom_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClasses = train_labels_sep[6][1].size\n",
    "\n",
    "## Create cnn model for detecting eye center\n",
    "mouse_ct_bottom_cnn = helper.convNetBuilder(numClasses, patchSize = [5,5], \n",
    "                                     featureMapLayers = [32,64,128,256,512],\n",
    "                                     numHiddenNodes = 600, numNNLayer = 5,\n",
    "                                     imageWidth = 96, poolingSize = 2,\n",
    "                                     train_dropout_rate = [0.2,0.5])\n",
    "\n",
    "train_data_sep[6] = helper.load_2d_images(train_data_sep[6], 96)\n",
    "dev_data_sep[6] = helper.load_2d_images(dev_data_sep[6], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mouse_ct_bottom_train_result, mouse_ct_bottom_val_result = mouse_ct_bottom_cnn.SGD(train_data_sep[6],\n",
    "                                                          train_labels_sep[6],\n",
    "                                                          update_rule=\"rmsprop\",\n",
    "                                                          epochs = 30,\n",
    "                                                          miniBatchSize = 100,\n",
    "                                                          learning_rate = 0.03,\n",
    "                                                          validation = [dev_data_sep[6],\n",
    "                                                                        dev_labels_sep[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)                  Output Shape              Param #          Param Shape      \n",
      "==========================================================================================\n",
      "conv2d_1 (Conv2D)             (None, 32, 96, 96)        800              (32, 1, 5, 5)    \n",
      "maxpooling2d_1 (Pooling)      (None, 32, 48, 48)        0                                 \n",
      "dropout_1 (Dropout)           (None, 32, 48, 48)        0                                 \n",
      "conv2d_2 (Conv2D)             (None, 64, 48, 48)        51200            (64, 32, 5, 5)   \n",
      "maxpooling2d_2 (Pooling)      (None, 64, 24, 24)        0                                 \n",
      "dropout_2 (Dropout)           (None, 64, 24, 24)        0                                 \n",
      "conv2d_3 (Conv2D)             (None, 128, 24, 24)       204800           (128, 64, 5, 5)  \n",
      "maxpooling2d_3 (Pooling)      (None, 128, 12, 12)       0                                 \n",
      "dropout_3 (Dropout)           (None, 128, 12, 12)       0                                 \n",
      "conv2d_4 (Conv2D)             (None, 256, 12, 12)       819200           (256, 128, 5, 5) \n",
      "maxpooling2d_4 (Pooling)      (None, 256, 6, 6)         0                                 \n",
      "dropout_4 (Dropout)           (None, 256, 6, 6)         0                                 \n",
      "conv2d_5 (Conv2D)             (None, 512, 6, 6)         3276800          (512, 256, 5, 5) \n",
      "maxpooling2d_5 (Pooling)      (None, 512, 3, 3)         0                                 \n",
      "dropout_5 (Dropout)           (None, 512, 3, 3)         0                                 \n"
     ]
    }
   ],
   "source": [
    "print(\"Layer (type)                  Output Shape              Param #          Param Shape      \")\n",
    "print(\"==========================================================================================\")\n",
    "print(\"conv2d_1 (Conv2D)             (None, 32, 96, 96)        800              (32, 1, 5, 5)    \")\n",
    "print(\"maxpooling2d_1 (Pooling)      (None, 32, 48, 48)        0                                 \")\n",
    "print(\"dropout_1 (Dropout)           (None, 32, 48, 48)        0                                 \") \n",
    "print(\"conv2d_2 (Conv2D)             (None, 64, 48, 48)        51200            (64, 32, 5, 5)   \")\n",
    "print(\"maxpooling2d_2 (Pooling)      (None, 64, 24, 24)        0                                 \")\n",
    "print(\"dropout_2 (Dropout)           (None, 64, 24, 24)        0                                 \")\n",
    "print(\"conv2d_3 (Conv2D)             (None, 128, 24, 24)       204800           (128, 64, 5, 5)  \")\n",
    "print(\"maxpooling2d_3 (Pooling)      (None, 128, 12, 12)       0                                 \")\n",
    "print(\"dropout_3 (Dropout)           (None, 128, 12, 12)       0                                 \")\n",
    "print(\"conv2d_4 (Conv2D)             (None, 256, 12, 12)       819200           (256, 128, 5, 5) \")\n",
    "print(\"maxpooling2d_4 (Pooling)      (None, 256, 6, 6)         0                                 \")\n",
    "print(\"dropout_4 (Dropout)           (None, 256, 6, 6)         0                                 \")\n",
    "print(\"conv2d_5 (Conv2D)             (None, 512, 6, 6)         3276800          (512, 256, 5, 5) \")\n",
    "print(\"maxpooling2d_5 (Pooling)      (None, 512, 3, 3)         0                                 \")\n",
    "print(\"dropout_5 (Dropout)           (None, 512, 3, 3)         0                                 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
